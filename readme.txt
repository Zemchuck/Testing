BANKING77 INTENT CLASSIFIER â€“Â MLOPS LAB

This repository contains the code and artefacts created while completing LabÂ 4 of the AGHâ€¯MLOps course.  The goal is to fineâ€‘tune a DistilBERT model on the Banking77 dataset, evaluate it, probe the model for weaknesses and generate explanations â€” all with proper experiment tracking and automation.

CONTENTS

â€¢ Core scripts
- train_classifier.py             â€“ fineâ€‘tunes DistilBERT on Banking77 and logs everything to MLflow
- evaluate_model.py               â€“ evaluates the trained model, writes metrics and saves plots
- exploration.py                  â€“ exploratory data analysis (class distribution, text length â€¦)
- captum_explainability.py        â€“ tokenâ€‘level explanations generated with Captum
- giskard_behavioral_testing.py   â€“ behavioural scan and HTML report via Giskard

â€¢ Artefacts & outputs
- model/                          â€“ final fineâ€‘tuned model (ðŸ¤— Transformers format)
- results/                        â€“ intermediate training checkpoints
- *.png                           â€“ plots created during EDA / evaluation
- giskard_behavioural_report.html â€“ standalone behaviouralâ€‘scan report

â€¢ Experiment tracking
- mlruns/                         â€“ local MLflow tracking directory (ignored by Git)

â€¢ Project plumbing
- pyproject.toml                  â€“ dependencies and project metadata
- .gitignore                      â€“ ignore rules (Python caches, venv, MLflow artefacts â€¦)
- .venv/                          â€“ local virtual environment (not committed)

QUICK START

Clone the repo and enter it:
git clone <yourâ€‘forkâ€‘url>
cd Testing

Create a reproducible environment with uv (or use venv/conda):
uv venv && source .venv/bin/activate
uv pip install -r pyproject.toml

(OptionÂ A) Train from scratch:
uv run python train_classifier.py       # produces checkpoints + MLflow logs
(OptionÂ B) Skip training and use the ready model stored in ./model

Evaluate and generate visualisations:
uv run python evaluate_model.py
open class_distribution.png
open f1_per_class.png

Explainability (Captum):
uv run python captum_explainability.py

Behavioural testing (Giskard):
uv run python giskard_behavioral_testing.py
open giskard_behavioural_report.html

Notes:
â€¢ All scripts autoâ€‘detect CUDA and fall back to CPU.
â€¢ Launch  "mlflow ui -p 5000"  to browse experiment runs stored in mlruns/.

KEY RESULTS

Accuracy (test set)  : 94.6Â %
MacroÂ F1 (test set)  : 0.946
Perâ€‘class scores     : see f1_per_class.png

BEHAVIOURAL SCAN (GISKARD)

â€¢ HTML report: giskard_behavioural_report.html
â€¢ 1 medium performanceâ€‘bias issue on slice "money" (precision â€“6.6Â %).
â€¢ No critical over/underâ€‘confidence or ethical bias findings.

PROJECT SETUP & CONVENTIONS

â€¢ PythonÂ 3.11, managed with uv for deterministic builds.
â€¢ Coding style enforced via ruff + black (optional preâ€‘commit hooks).
â€¢ Experiments tracked with MLflow (data stored in mlruns/, ignored by Git).
â€¢ Large artefacts (models, checkpoints) are committed for convenience; configure GitÂ LFS if size becomes an issue.

REâ€‘TRAINING WITH CUSTOM HYPERâ€‘PARAMETERS

train_classifier.py exposes most HuggingÂ Face TrainingArguments via CLI.  Example:

uv run python train_classifier.py --learning_rate 2e-5 --num_train_epochs 5 --per_device_train_batch_size 16

A new MLflow run will appear automatically.

LICENSE &Â CITATION

Code Â©Â 2025â€¯MLOpsÂ Lab, AGH â€” MIT License.
Banking77 dataset Â©Â PolyAI.